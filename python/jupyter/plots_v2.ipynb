{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import fnmatch\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "from scipy import stats\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set_style('ticks')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.size': 7,\n",
    "    'axes.labelsize': 7,\n",
    "    'axes.titlesize': 7,\n",
    "    'xtick.labelsize': 6,\n",
    "    'ytick.labelsize': 6,\n",
    "    'legend.fontsize': 6,\n",
    "    'lines.linewidth': 1.0,\n",
    "    'lines.markersize': 4.0,\n",
    "    'axes.linewidth': 0.8,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 600,\n",
    "    'figure.figsize': [3.5, 2.625],\n",
    "    'figure.constrained_layout.use': True,\n",
    "    'axes.xmargin': 0.05,\n",
    "    'axes.ymargin': 0.05,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    # Reduce margins around the entire figure DOESNT DO ANYTHING\n",
    "    #'figure.constrained_layout.h_pad': 0.1,  # Horizontal padding\n",
    "    #'figure.constrained_layout.w_pad': 0.1,  # Vertical padding\n",
    "    # Reduce space between title and plot\n",
    "    'axes.titlepad': 1,         # Distance between title and plot\n",
    "    # Reduce space between labels and ticks\n",
    "    'axes.labelpad': 1,         # Distance between axis label and ticks\n",
    "    # Reduce tick parameters\n",
    "    'xtick.major.pad': 1,       # Distance between tick and tick label\n",
    "    'ytick.major.pad': 1,\n",
    "    \n",
    "    # Adjust subplot spacing\n",
    "    'figure.subplot.top': 0.95,    # Top margin\n",
    "    'figure.subplot.bottom': 0.15, # Bottom margin\n",
    "    'figure.subplot.left': 0.15,   # Left margin\n",
    "    'figure.subplot.right': 0.95   # Right margin\n",
    "})\n",
    "\n",
    "# Global plot saving configuration\n",
    "SAVE_PLOTS = True  # Switch this to True to enable saving\n",
    "PLOT_DIR = Path('/var/home/thorben/git/bachelor_thesis_docs/written_bachelor_thesis/bilder/plots')  # Directory for saved plots\n",
    "RESULT_PATH = \"/var/home/thorben/git/bachelor_thesis_docs/jupiter/results/\"\n",
    "\n",
    "def save_plot(name, fig=None):\n",
    "    \"\"\"Helper function to save plots if SAVE_PLOTS is enabled\n",
    "    \n",
    "    Args:\n",
    "        name: Name of the plot (will be used as filename)\n",
    "        fig: Figure to save (defaults to current figure)\n",
    "    \"\"\"\n",
    "    if SAVE_PLOTS:\n",
    "        # Create plots directory if it doesn't exist\n",
    "        PLOT_DIR.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Get current figure if none provided\n",
    "        if fig is None:\n",
    "            fig = plt.gcf()\n",
    "            \n",
    "        # Save with timestamp to avoid overwrites\n",
    "        filename = PLOT_DIR / f\"{name}.png\"\n",
    "        fig.savefig(filename, bbox_inches='tight', dpi=600)\n",
    "        print(f\"Saved plot to {filename}\")\n",
    "\n",
    "# Load benchmark results\n",
    "def load_benchmark_results(files):\n",
    "    results = {}\n",
    "    if isinstance(files, str):\n",
    "        files = [files]\n",
    "    for file in files:\n",
    "        path = Path(RESULT_PATH + file)\n",
    "        with open(path, 'r') as f:\n",
    "            results[path.stem] = json.load(f)\n",
    "    return results\n",
    "\n",
    "def get_title_metadata(result_key: str, with_unterscores=False):\n",
    "    if(not with_unterscores): result_key.replace(\"_\", \" \")\n",
    "    return result_key.replace(\"benchmark_\", '')\n",
    "\n",
    "def calculate_method_memory(method: str, num_vectors, vector_dim):\n",
    "    \"\"\"Calculate theoretical memory usage for a method\n",
    "    Args:\n",
    "        method: Name of the method\n",
    "        num_vectors: Number of vectors in the dataset\n",
    "        vector_dim: Dimension of each vector\n",
    "    Returns:\n",
    "        Memory usage in bytes\n",
    "    \"\"\"\n",
    "    # Base memory calculation\n",
    "    if method.startswith('float16'):\n",
    "        vector_memory = num_vectors * vector_dim * 2\n",
    "    elif method.startswith('float') or method.startswith('avx2'):\n",
    "        vector_memory = num_vectors * vector_dim * 4  # 4 bytes per float\n",
    "    elif method.startswith('binary'):\n",
    "        vector_memory = num_vectors * ((vector_dim + 7) // 8)  # Round up to nearest byte\n",
    "    elif method.startswith('int8') or method.startswith('mf'):\n",
    "        vector_memory = num_vectors * vector_dim  # 1 byte per value\n",
    "    elif method.startswith('pca'):\n",
    "        reduction_factor = int(method[3:]) if method[3:].isdigit() else 1\n",
    "        reduced_dim = vector_dim // reduction_factor\n",
    "        vector_memory = num_vectors * reduced_dim * 4  # Still using floats after PCA\n",
    "    elif method.startswith('twostep'):\n",
    "        binary_memory = num_vectors * ((vector_dim + 7) // 8)  # Binary vectors\n",
    "        float_memory = num_vectors * vector_dim * 4  # Float vectors\n",
    "        vector_memory = binary_memory + float_memory\n",
    "    elif method.startswith('ts_mf'):\n",
    "        mf_memory = calculate_method_memory('mf', num_vectors, vector_dim)\n",
    "        binary_memory = calculate_method_memory('binary', num_vectors, vector_dim)\n",
    "        vector_memory = mf_memory + binary_memory\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # Add overhead for index structures\n",
    "    #if method.startswith('twostep'):\n",
    "        #overhead = num_vectors * 8  # Additional index for binary search\n",
    "    if method.startswith('pca'):\n",
    "        reduction_factor = int(method[3:]) if method[3:].isdigit() else 1\n",
    "        reduced_dim = vector_dim // reduction_factor\n",
    "        overhead = vector_dim * reduced_dim * 4  # PCA transformation matrix\n",
    "    else:\n",
    "        #overhead = num_vectors * 4  # Basic index overhead\n",
    "        overhead = 0\n",
    "    \n",
    "    return vector_memory + overhead\n",
    "\n",
    "def calculate_bandwidth_memory(method: str, num_vectors, vector_dim, k):\n",
    "    \"\"\"Calculate memory accessed during search for bandwidth calculation.\n",
    "    \n",
    "    Args:\n",
    "        method: Name of the method\n",
    "        num_vectors: Number of vectors in dataset\n",
    "        vector_dim: Dimension of vectors\n",
    "        k: Number of neighbors to retrieve\n",
    "    Returns:\n",
    "        Memory accessed in bytes\n",
    "    \"\"\"\n",
    "    if method.startswith('float16'):\n",
    "        memory_bytes = num_vectors * vector_dim * 2\n",
    "    elif method.startswith('float') or method.startswith('avx2'):\n",
    "        # Access all float vectors\n",
    "        memory_bytes = num_vectors * vector_dim * 4\n",
    "    elif method.startswith('binary'):\n",
    "        # Access all binary vectors\n",
    "        memory_bytes = num_vectors * ((vector_dim + 7) // 8)\n",
    "    elif method.startswith('int8') or method.startswith('mf'):\n",
    "        # Access all int8 vectors\n",
    "        memory_bytes = num_vectors * vector_dim\n",
    "    elif method.startswith('pca'):\n",
    "        # Access reduced dimension float vectors\n",
    "        reduction_factor = int(method[3:]) if method[3:].isdigit() else 1\n",
    "        reduced_dim = vector_dim // reduction_factor\n",
    "        memory_bytes = num_vectors * reduced_dim * 4\n",
    "    elif method.startswith('twostep_rf'):\n",
    "        # First phase: access all binary vectors\n",
    "        binary_memory = num_vectors * ((vector_dim + 7) // 8)\n",
    "        \n",
    "        # Second phase: access top-k * rf float vectors\n",
    "        rf = int(method.replace('twostep_rf', ''))\n",
    "        float_memory = k * rf * vector_dim * 4\n",
    "        \n",
    "        memory_bytes = binary_memory + float_memory\n",
    "    elif method.startswith('ts_mf_rf'):\n",
    "        binary_memory = num_vectors * ((vector_dim + 7) // 8)\n",
    "        rf = int(method.replace('ts_mf_rf', ''))\n",
    "        mf_memory = k * rf * vector_dim * 1\n",
    "        memory_bytes = binary_memory + mf_memory\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    return memory_bytes\n",
    "\n",
    "# Load your data\n",
    "results = load_benchmark_results([\n",
    "    'benchmark_dim1024_k100_q.json',\n",
    "    'benchmark_dim1024_k100_re.json',\n",
    "    'benchmark_dim1024_k25_q.json',\n",
    "    'benchmark_dim1024_k10_q.json',\n",
    "    'benchmark_dim768_k100_q.json',\n",
    "    'benchmark_dim768_k100_re.json',\n",
    "    'benchmark_results_1733419058.json',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_comparison(results, result_key, skip_methods=None, enable_save_plot=False, name_postfix=\"\"):\n",
    "    if skip_methods is None:\n",
    "        skip_methods = []\n",
    "        \n",
    "    summary = results[result_key]['summary']\n",
    "    \n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'Method': method,\n",
    "            'Mean Time (ms)': stats['time_us']['mean'] / 1000,\n",
    "            'Std Time': stats['time_us']['std'] / 1000\n",
    "        }\n",
    "        for method, stats in summary.items()\n",
    "        #if method not in skip_methods\n",
    "        if not any(fnmatch.fnmatch(method, pattern) for pattern in skip_methods)\n",
    "    ])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    bars = sns.barplot(data=df, x='Method', y='Mean Time (ms)', \n",
    "                      errorbar=('ci', 68), ax=ax)\n",
    "    \n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('Search Time (ms)')\n",
    "    ax.set_title('Search Performance Comparison')\n",
    "    \n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    \n",
    "    for bar in bars.containers[0]:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.1f}', ha='center', va='bottom', fontsize=7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    skipped_methods = \"\"\n",
    "    for name in skip_methods:\n",
    "        skipped_methods += name\n",
    "    if enable_save_plot: save_plot(f'performance_comparison_{get_title_metadata(result_key=result_key, with_unterscores=True)}{name_postfix}')\n",
    "    plt.show()\n",
    "\n",
    "# Generate plot\n",
    "plot_performance_comparison(results, 'benchmark_dim1024_k100_q', skip_methods=['float', 'mf', 'float16', 'twostep_rf5', 'twostep_rf10', 'twostep_rf25', 'ts_mf_rf5', 'ts_mf_rf10', 'ts_mf_rf25'], enable_save_plot=True)\n",
    "plot_performance_comparison(results, 'benchmark_dim1024_k100_q', skip_methods=['float16'])\n",
    "plot_performance_comparison(results, 'benchmark_dim1024_k100_re', skip_methods=['float', 'mf', 'float16'])\n",
    "plot_performance_comparison(results, 'benchmark_dim1024_k100_re', skip_methods=['float16'])\n",
    "plot_performance_comparison(results, 'benchmark_dim768_k100_q', skip_methods=['float', 'mf', 'float16', 'twostep_rf5', 'twostep_rf10', 'twostep_rf25', 'ts_mf_rf5', 'ts_mf_rf10', 'ts_mf_rf25'], enable_save_plot=True)\n",
    "plot_performance_comparison(results, 'benchmark_results_1733419058', skip_methods=['float', 'mf', 'float16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_heatmap(results, result_key, only_methods=None, enable_save_plot=False, name_postfix=\"\"):\n",
    "    metrics = []\n",
    "    \n",
    "    for method, stats in results[result_key]['summary'].items():\n",
    "        if method != 'float' and (only_methods == None or any(fnmatch.fnmatch(method, pattern) for pattern in only_methods)):\n",
    "            ndcg = stats.get('ndcg', {}).get('mean', 1.0)\n",
    "            jaccard = stats.get('jaccard_index', {}).get('mean', 1.0)\n",
    "            overlap = stats.get('overlap', {}).get('mean', results[result_key]['metadata']['k'])\n",
    "            metrics.append({\n",
    "                'Method': method,\n",
    "                'NDCG': ndcg,\n",
    "                'Jaccard': jaccard,\n",
    "                'Overlap': overlap / results[result_key]['metadata']['k']\n",
    "            })\n",
    "    \n",
    "    if not metrics:\n",
    "        print(\"No valid metrics found to create heatmap\")\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(metrics).set_index('Method')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    sns.heatmap(df, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "               vmin=0, vmax=1, center=0.5, ax=ax,\n",
    "               annot_kws={'size': 7},\n",
    "               cbar=False)  # Disable colorbar\n",
    "    \n",
    "    ax.set_title('Accuracy Metrics Comparison')\n",
    "    plt.tight_layout()\n",
    "    if enable_save_plot: save_plot(f'accuracy_heatmap_{get_title_metadata(result_key=result_key, with_unterscores=True)}{name_postfix}')\n",
    "    plt.show()\n",
    "\n",
    "# Generate plot\n",
    "plot_accuracy_heatmap(results, 'benchmark_dim1024_k100_q', only_methods=['avx2','binary', 'int8', 'float16', 'mf', 'pca*'], enable_save_plot=True)\n",
    "plot_accuracy_heatmap(results, 'benchmark_dim1024_k100_q', only_methods=['twostep*','ts*'], enable_save_plot=True, name_postfix=\"_twostep\")\n",
    "#plot_accuracy_heatmap(results, 'benchmark_dim1024_k100_re')\n",
    "#plot_accuracy_heatmap(results, 'benchmark_results_1733419058', only_methods=['twostep*','ts*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pareto_frontier(results, result_key, skip_methods = ['float'], enable_save_plot=False, name_postfix=\"\"):\n",
    "    \"\"\"Create a scatter plot showing the trade-off between speed and accuracy.\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary containing benchmark results\n",
    "        result_key: Key of the benchmark results to plot\n",
    "        skip_methods: List of method names to exclude\n",
    "    \"\"\"\n",
    "    summary = results[result_key]['summary']\n",
    "    \n",
    "    # Collect data points\n",
    "    data = []\n",
    "    for method, stats in summary.items():\n",
    "        if method not in skip_methods:\n",
    "            data.append({\n",
    "                'Method': method,\n",
    "                'Time (ms)': stats['time_us']['mean']/1000,\n",
    "                'NDCG': stats.get('ndcg', {}).get('mean', 1.0)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Create scatter plot\n",
    "    sns.scatterplot(data=df, x='Time (ms)', y='NDCG', s=50)\n",
    "    \n",
    "    # Add method labels next to points\n",
    "    for _, row in df.iterrows():\n",
    "        if(row['Method'] == 'twostep_rf25'): textpos = (5, -2)\n",
    "        elif(row['Method'] == 'twostep_rf50'): textpos = (5, 3)\n",
    "        elif(row['Method'] == 'twostep_rf2'): textpos = (5, 3)\n",
    "        else: textpos = (5, -5)\n",
    "        ax.annotate(row['Method'], \n",
    "                   (row['Time (ms)'], row['NDCG']),\n",
    "                   xytext=textpos, \n",
    "                   textcoords='offset points',\n",
    "                   fontsize=6)\n",
    "    \n",
    "    ax.set_title('Speed-Accuracy Trade-off Analysis')\n",
    "    ax.set_xlabel('Search Time (ms)')\n",
    "    ax.set_ylabel('NDCG Score')\n",
    "    \n",
    "    # Ensure NDCG axis starts at 0\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlim(0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if enable_save_plot: save_plot(f'speed_vs_accuracy_{get_title_metadata(result_key=result_key, with_unterscores=True)}{name_postfix}')\n",
    "    plt.show()\n",
    "\n",
    "# Generate plot\n",
    "plot_pareto_frontier(results, 'benchmark_dim1024_k100_q', skip_methods=['float', 'float16', 'mf', 'twostep_rf5', 'twostep_rf10', 'twostep_rf25', 'ts_mf_rf5', 'ts_mf_rf10', 'ts_mf_rf25'], enable_save_plot=True)\n",
    "plot_pareto_frontier(results, 'benchmark_dim1024_k100_re', skip_methods=['float', 'float16', 'mf'])\n",
    "plot_pareto_frontier(results, 'benchmark_results_1733419058', skip_methods=['float', 'float16', 'mf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pareto_frontier_memory(results, result_key, skip_methods = ['float'], enable_save_plot=False, name_postfix=\"\"):\n",
    "    \"\"\"Create a scatter plot showing the trade-off between memory and accuracy.\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary containing benchmark results\n",
    "        result_key: Key of the benchmark results to plot\n",
    "        skip_methods: List of method names to exclude\n",
    "    \"\"\"\n",
    "    summary = results[result_key]['summary']\n",
    "    num_vectors = results[result_key]['metadata']['num_vectors']\n",
    "    vector_dim = results[result_key]['metadata']['vector_dim']\n",
    "    \n",
    "    # Collect data points\n",
    "    data = []\n",
    "    for method, stats in summary.items():\n",
    "        if method not in skip_methods:\n",
    "            data.append({\n",
    "                'Method': method,\n",
    "                'Memory (GB)': calculate_method_memory(method, num_vectors, vector_dim)/1024/1024/1024,\n",
    "                'NDCG': stats.get('ndcg', {}).get('mean', 1.0)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Create scatter plot\n",
    "    sns.scatterplot(data=df, x='Memory (GB)', y='NDCG', s=50)\n",
    "    \n",
    "    # Add method labels next to points\n",
    "    for _, row in df.iterrows():\n",
    "        if(row['Method'] == 'twostep_rf25'): textpos = (5, -2)\n",
    "        elif(row['Method'] == 'twostep_rf50'): textpos = (5, 3)\n",
    "        elif row['Method'] in ['avx2', 'mf']: textpos = (-12,-5)\n",
    "        else: textpos = (5, -5)\n",
    "        ax.annotate(row['Method'], \n",
    "                   (row['Memory (GB)'], row['NDCG']),\n",
    "                   xytext=textpos, \n",
    "                   textcoords='offset points',\n",
    "                   fontsize=6)\n",
    "    \n",
    "    ax.set_title('Memory-Accuracy Trade-off Analysis')\n",
    "    ax.set_xlabel('Used Memory (GB)')\n",
    "    ax.set_ylabel('NDCG Score')\n",
    "    \n",
    "    # Ensure NDCG axis starts at 0\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlim(0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if enable_save_plot: save_plot(f'memory_vs_accuracy_{get_title_metadata(result_key=result_key, with_unterscores=True)}{name_postfix}')\n",
    "    plt.show()\n",
    "\n",
    "# Generate plot\n",
    "plot_pareto_frontier_memory(results, 'benchmark_dim1024_k100_q', skip_methods=['float', 'ts_mf_rf5', 'ts_mf_rf10', '_ts_mf_rf25', 'ts_mf_rf50'], enable_save_plot=True)\n",
    "plot_pareto_frontier_memory(results, 'benchmark_dim1024_k100_re', skip_methods=['float'])\n",
    "plot_pareto_frontier_memory(results, 'benchmark_results_1733419058', skip_methods=['float', 'ts_mf_rf5', 'ts_mf_rf10', '_ts_mf_rf25', 'ts_mf_rf50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_twostep_comparison(results, result_key, enable_save_plot=False, name_postfix=\"\"):\n",
    "    \"\"\"Plot comparison of different rescoring factors for twostep methods.\n",
    "    \n",
    "    Creates two plots:\n",
    "    1. Search time vs rescoring factor\n",
    "    2. Search quality (NDCG and Jaccard) vs rescoring factor\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing benchmark results\n",
    "        result_key: Key of the benchmark results to plot\n",
    "    \"\"\"\n",
    "    # Get twostep methods\n",
    "    twostep_methods = [m for m in results[result_key]['summary'].keys() \n",
    "                      if m.startswith('twostep_rf')]\n",
    "    twostep_methods += [m for m in results[result_key]['summary'].keys() \n",
    "                      if m.startswith('ts_mf_rf')]\n",
    "    \n",
    "    if not twostep_methods:\n",
    "        print(\"No twostep methods found in results\")\n",
    "        return\n",
    "    \n",
    "    # Collect data\n",
    "    data = []\n",
    "    for method in twostep_methods:\n",
    "        if method.startswith('twostep_rf'): rf = int(method.replace('twostep_rf', ''))\n",
    "        else: rf = int(method.replace('ts_mf_rf', ''))\n",
    "        stats = results[result_key]['summary'][method]\n",
    "        data.append({\n",
    "            'Rescoring Factor': rf,\n",
    "            'Mean Time (ms)': stats['time_us']['mean'] / 1000,\n",
    "            'Mean NDCG': stats.get('ndcg', {}).get('mean', 1.0),\n",
    "            'Jaccard Index': stats.get('jaccard_index', {}).get('mean', 1.0)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data).sort_values('Rescoring Factor')\n",
    "    \n",
    "    # Plot 1: Search Time\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    \n",
    "    sns.lineplot(data=df, x='Rescoring Factor', y='Mean Time (ms)', \n",
    "                marker='o', ax=ax1)\n",
    "    \n",
    "    ax1.set_title('Search Time vs Rescoring Factor')\n",
    "    ax1.set_xlabel('Rescoring Factor')\n",
    "    ax1.set_ylabel('Mean Search Time (ms)')\n",
    "    \n",
    "    # Ensure x-axis shows all integer rescoring factors\n",
    "    ax1.set_xticks(df['Rescoring Factor'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if enable_save_plot: save_plot(f'twostep_time_{result_key}{name_postfix}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 2: Search Quality\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    \n",
    "    sns.lineplot(data=df, x='Rescoring Factor', y='Mean NDCG', \n",
    "                marker='o', label='NDCG', ax=ax2)\n",
    "    sns.lineplot(data=df, x='Rescoring Factor', y='Jaccard Index', \n",
    "                marker='o', label='Jaccard', ax=ax2)\n",
    "    \n",
    "    ax2.set_title('Search Quality vs Rescoring Factor')\n",
    "    ax2.set_xlabel('Rescoring Factor')\n",
    "    ax2.set_ylabel('Score')\n",
    "    \n",
    "    # Ensure x-axis shows all integer rescoring factors\n",
    "    ax2.set_xticks(df['Rescoring Factor'])\n",
    "    \n",
    "    # Set y-axis limits for scores\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    \n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if enable_save_plot: save_plot(f'twostep_comparison_{get_title_metadata(result_key=result_key, with_unterscores=True)}{name_postfix}')\n",
    "    plt.show()\n",
    "\n",
    "# Generate plots\n",
    "plot_twostep_comparison(results, 'benchmark_dim1024_k100_q')\n",
    "plot_twostep_comparison(results, 'benchmark_dim1024_k25_q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_twostep_comparison_combined(results, result_key, show_method='twostep_rf', enable_save_plot=False, name_postfix=\"\"):\n",
    "    \"\"\"Plot combined comparison of time and quality metrics for twostep methods.\n",
    "    \n",
    "    Creates a single plot with dual y-axes showing:\n",
    "    - Search time vs rescoring factor (left axis)\n",
    "    - Search quality (NDCG and Jaccard) vs rescoring factor (right axis)\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing benchmark results\n",
    "        result_key: Key of the benchmark results to plot\n",
    "    \"\"\"\n",
    "    # Get twostep methods\n",
    "    twostep_methods = [m for m in results[result_key]['summary'].keys() \n",
    "                      if m.startswith(show_method)]\n",
    "    \n",
    "    if not twostep_methods:\n",
    "        print(\"No twostep methods found in results\")\n",
    "        return\n",
    "    \n",
    "    # Collect data\n",
    "    data = []\n",
    "    for method in twostep_methods:\n",
    "        rf = int(method.replace(show_method, ''))\n",
    "        stats = results[result_key]['summary'][method]\n",
    "        data.append({\n",
    "            'Rescoring Factor': rf,\n",
    "            'Mean Time (ms)': stats['time_us']['mean'] / 1000,\n",
    "            'Mean NDCG': stats.get('ndcg', {}).get('mean', 1.0),\n",
    "            'Jaccard Index': stats.get('jaccard_index', {}).get('mean', 1.0)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data).sort_values('Rescoring Factor')\n",
    "    \n",
    "    # Create figure with two y-axes\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plot time on left axis\n",
    "    line1 = ax1.plot(df['Rescoring Factor'], df['Mean Time (ms)'], \n",
    "                     marker='o', color='black', label='Time')\n",
    "    ax1.set_xlabel('Rescoring Factor')\n",
    "    ax1.set_ylabel('Mean Search Time (ms)')\n",
    "    \n",
    "    # Plot quality metrics on right axis\n",
    "    line2 = ax2.plot(df['Rescoring Factor'], df['Mean NDCG'], \n",
    "                     marker='s', linestyle='--', color='gray', label='NDCG')\n",
    "    line3 = ax2.plot(df['Rescoring Factor'], df['Jaccard Index'], \n",
    "                     marker='^', linestyle=':', color='lightgray', label='Jaccard')\n",
    "    ax2.set_ylabel('Score')\n",
    "    \n",
    "    # Set y-axis limits for scores\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Ensure x-axis shows all integer rescoring factors\n",
    "    ax1.set_xticks(df['Rescoring Factor'])\n",
    "    \n",
    "    # Add combined legend\n",
    "    lines = line1 + line2 + line3\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='center right')\n",
    "    \n",
    "    plt.title('Twostep Method Performance' + ' ' + show_method)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if enable_save_plot: save_plot(f'twostep_comparison_combined_{result_key}{name_postfix}')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# plot_twostep_comparison(results_dict, 'benchmark_results_example')\n",
    "plot_twostep_comparison_combined(results, 'benchmark_dim1024_k100_q', enable_save_plot=True)\n",
    "plot_twostep_comparison_combined(results, 'benchmark_dim1024_k100_q', show_method='ts_mf_rf', enable_save_plot=True, name_postfix=\"_mf\")\n",
    "#plot_twostep_comparison_combined(results, 'benchmark_dim1024_k25_q')\n",
    "#plot_twostep_comparison_combined(results, 'benchmark_results_1733419058')\n",
    "#plot_twostep_comparison_combined(results, 'benchmark_results_1733419058', show_method='ts_mf_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_boxplots(results, result_key, metric='ndcg', skip_methods=None, enable_save_plot=False, name_postfix=\"\"):\n",
    "    \"\"\"Create box plots for NDCG or Jaccard Index distributions.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing benchmark results\n",
    "        result_key: Key of the benchmark results to plot\n",
    "        metric: Which metric to plot ('ndcg' or 'jaccard')\n",
    "        skip_methods: List of method names to skip in the plot\n",
    "    \"\"\"\n",
    "    if skip_methods is None:\n",
    "        skip_methods = []\n",
    "        \n",
    "    # Configure based on metric\n",
    "    metric_config = {\n",
    "        'ndcg': {'key': 'ndcg', 'title': 'NDCG', 'ylabel': 'NDCG Score'},\n",
    "        'jaccard': {'key': 'jaccard_index', 'title': 'Jaccard Index', 'ylabel': 'Jaccard Index'}\n",
    "    }\n",
    "    \n",
    "    if metric not in metric_config:\n",
    "        raise ValueError(f\"Metric must be one of {list(metric_config.keys())}\")\n",
    "    \n",
    "    config = metric_config[metric]\n",
    "    \n",
    "    # Collect metric data from each run\n",
    "    plot_data = []\n",
    "    methods = []\n",
    "    \n",
    "    for run in results[result_key]['runs']:\n",
    "        for search in run['searches']:\n",
    "            method = search['method']\n",
    "            if method != 'float' and not any(fnmatch.fnmatch(method, pattern) for pattern in skip_methods):\n",
    "                if method not in methods: methods.append(method)\n",
    "                try:\n",
    "                    metrics = search['metrics']\n",
    "                    value = metrics[config['key']]\n",
    "                    plot_data.append({\n",
    "                        'Method': method,\n",
    "                        'Value': value\n",
    "                    })\n",
    "                except KeyError:\n",
    "                    print(f\"Warning: Missing {config['key']} metric for method {method}\")\n",
    "                    continue\n",
    "    \n",
    "    if not plot_data:\n",
    "        print(f\"No valid {metric} data found to plot\")\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    # Create boxplot\n",
    "    sns.boxplot(data=df, x='Method', y='Value',\n",
    "                showfliers=True,\n",
    "                width=0.6,\n",
    "                linewidth=0.8,\n",
    "                fliersize=1.0,\n",
    "                fill=False,\n",
    "                flierprops={\"marker\": \"x\", 'markerfacecolor': 'black', 'markeredgecolor': 'black'},\n",
    "                boxprops={'color': 'black'},  # Set box color to black\n",
    "                whiskerprops={'color': 'black'},  # Match whisker color\n",
    "                medianprops={'color': 'black'},  # Match median line color\n",
    "                capprops={'color': 'black'},\n",
    "                notch=False)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.title(f'{config[\"title\"]} Score Distributions')\n",
    "    plt.ylabel(config['ylabel'])\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylim(0, 1.025)  # Both metrics are between 0 and 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot if enabled\n",
    "    if enable_save_plot: save_plot(f'{metric}_boxplots_{get_title_metadata(result_key=result_key, with_unterscores=True)}{name_postfix}')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistical summary\n",
    "    print(f\"\\n{config['title']} Statistical Summary:\")\n",
    "    summary = df.groupby('Method')['Value'].describe()\n",
    "    print(summary.round(4).to_string())\n",
    "\n",
    "# Generate plots\n",
    "plot_metric_boxplots(results, 'benchmark_dim1024_k100_q', metric='ndcg', skip_methods=['ts_mf*', 'twostep_rf*'], enable_save_plot=True)\n",
    "plot_metric_boxplots(results, 'benchmark_dim1024_k100_q', metric='jaccard', skip_methods=['ts_mf*', 'twostep_rf*'], enable_save_plot=True)\n",
    "plot_metric_boxplots(results, 'benchmark_dim1024_k100_q', metric='ndcg', skip_methods=['avx2', 'binary', 'int8', 'float16', 'mf', 'pca*'], name_postfix=\"_twostep\", enable_save_plot=True)\n",
    "plot_metric_boxplots(results, 'benchmark_dim1024_k100_q', metric='jaccard', skip_methods=['avx2', 'binary', 'int8', 'float16', 'mf', 'pca*'], name_postfix=\"_twostep\", enable_save_plot=True)\n",
    "\n",
    "plot_metric_boxplots(results, 'benchmark_dim1024_k100_re', metric='ndcg', skip_methods=['ts_mf*', 'twostep_rf*'], enable_save_plot=True)\n",
    "plot_metric_boxplots(results, 'benchmark_dim1024_k100_re', metric='jaccard', skip_methods=['ts_mf*', 'twostep_rf*'], enable_save_plot=True)\n",
    "plot_metric_boxplots(results, 'benchmark_dim1024_k100_re', metric='ndcg', skip_methods=['avx2', 'binary', 'int8', 'float16', 'mf', 'pca*'], name_postfix=\"_twostep\", enable_save_plot=True)\n",
    "plot_metric_boxplots(results, 'benchmark_dim1024_k100_re', metric='jaccard', skip_methods=['avx2', 'binary', 'int8', 'float16', 'mf', 'pca*'], name_postfix=\"_twostep\", enable_save_plot=True)\n",
    "#plot_metric_boxplots(results, 'benchmark_dim1024_k10_q', metric='ndcg', skip_methods=['ts_mf*', 'twostep_rf*'])\n",
    "#plot_metric_boxplots(results, 'benchmark_dim1024_k100_re', metric='ndcg')\n",
    "#plot_metric_boxplots(results, 'benchmark_dim1024_k100_re', metric='jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_vs_memory(results, result_key, enable_save_plot=False, name_postfix=\"\"):\n",
    "    \"\"\"Create a scatter plot showing performance vs memory usage trade-off.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing benchmark results\n",
    "        result_key: Key of the benchmark results to plot\n",
    "    \"\"\"\n",
    "    summary = results[result_key]['summary']\n",
    "    num_vectors = results[result_key]['metadata']['num_vectors']\n",
    "    vector_dim = results[result_key]['metadata']['vector_dim']\n",
    "    \n",
    "    # Collect data\n",
    "    data = []\n",
    "    baseline_time = summary['avx2']['time_us']['mean']\n",
    "    \n",
    "    for method, stats in summary.items():\n",
    "        if method != 'float':\n",
    "            memory_mb = calculate_method_memory(method, num_vectors, vector_dim) / (1024 * 1024)\n",
    "            speedup = baseline_time / stats['time_us']['mean']\n",
    "            ndcg = stats.get('ndcg', {}).get('mean', 1.0)\n",
    "            \n",
    "            data.append({\n",
    "                'Method': method,\n",
    "                'Memory (MB)': memory_mb,\n",
    "                'Speedup': speedup,\n",
    "                'NDCG': ndcg,\n",
    "                'Size': ndcg * ndcg * ndcg * 250  # Adjusted size scaling\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(df['Memory (MB)'], df['Speedup'], \n",
    "                s=df['Size'], alpha=0.6, \n",
    "                color='black')  # Single color (black)\n",
    "    \n",
    "    # Add method labels\n",
    "    for _, row in df.iterrows():\n",
    "        plt.annotate(row['Method'], \n",
    "                    (row['Memory (MB)'], row['Speedup']),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=8)\n",
    "    \n",
    "    plt.title('Performance vs Memory Usage')\n",
    "    plt.xlabel('Memory Usage (MB)')\n",
    "    plt.ylabel('Speedup vs Baseline (avx2)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot if enabled\n",
    "    if enable_save_plot: save_plot(f'performance_vs_memory_{get_title_metadata(result_key=result_key, with_unterscores=True)}{name_postfix}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Generate plot\n",
    "plot_performance_vs_memory(results, 'benchmark_dim1024_k100_q')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_table(results, result_key, num_variance=5, num_extreme=3, num_random=0, random_seed=42):\n",
    "    \"\"\"Create a table showing NDCG scores for different methods across interesting queries.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing benchmark results\n",
    "        result_key: Key of the benchmark results to plot\n",
    "        num_variance: Number of high-variance queries to show\n",
    "        num_extreme: Number of best/worst queries to show\n",
    "        num_random: Number of random queries to show\n",
    "        random_seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "    # Collect per-query performance data\n",
    "    query_data = []\n",
    "    \n",
    "    for run in results[result_key]['runs']:\n",
    "        query_text = run['query_text']\n",
    "        for search in run['searches']:\n",
    "            if search['method'] != 'float':  # Skip baseline\n",
    "                query_data.append({\n",
    "                    'Query': query_text,\n",
    "                    'Method': search['method'],\n",
    "                    'NDCG': search['metrics'].get('ndcg', 1.0)\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(query_data)\n",
    "    \n",
    "    # Create pivot table for calculating statistics\n",
    "    query_matrix = df.pivot_table(\n",
    "        values='NDCG',\n",
    "        index='Query',\n",
    "        columns='Method',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    # Calculate statistics for each query\n",
    "    query_stats = query_matrix.agg(['mean', 'std'], axis=1)\n",
    "    query_stats.columns = ['mean_ndcg', 'std_ndcg']\n",
    "    \n",
    "    # Select interesting queries\n",
    "    high_var_queries = query_stats.nlargest(num_variance, 'std_ndcg').index.tolist()\n",
    "    best_queries = query_stats.nlargest(num_extreme, 'mean_ndcg').index.tolist()\n",
    "    worst_queries = query_stats.nsmallest(num_extreme, 'mean_ndcg').index.tolist()\n",
    "    \n",
    "    # Select random queries\n",
    "    available_queries = list(set(query_matrix.index) - \n",
    "                           set(high_var_queries) - \n",
    "                           set(best_queries) - \n",
    "                           set(worst_queries))\n",
    "    random_queries = []\n",
    "    if num_random > 0 and available_queries:\n",
    "        random_queries = list(np.random.choice(available_queries, \n",
    "                                             size=min(num_random, len(available_queries)), \n",
    "                                             replace=False))\n",
    "    \n",
    "    # Combine and deduplicate queries\n",
    "    selected_queries = list(dict.fromkeys(\n",
    "        worst_queries + high_var_queries + best_queries + random_queries\n",
    "    ))\n",
    "    \n",
    "    # Filter matrix for selected queries\n",
    "    selected_matrix = query_matrix.loc[selected_queries]\n",
    "    \n",
    "    # Add statistics columns\n",
    "    selected_matrix['Mean'] = query_stats.loc[selected_queries, 'mean_ndcg']\n",
    "    selected_matrix['Std'] = query_stats.loc[selected_queries, 'std_ndcg']\n",
    "    \n",
    "    # Sort by mean NDCG\n",
    "    selected_matrix = selected_matrix.sort_values('Mean', ascending=True)\n",
    "    \n",
    "    # Create labels with query type and shortened text\n",
    "    query_labels = {}\n",
    "    for q in selected_matrix.index:\n",
    "        if q in worst_queries:\n",
    "            prefix = 'WORST - '\n",
    "        elif q in best_queries:\n",
    "            prefix = 'BEST - '\n",
    "        elif q in high_var_queries:\n",
    "            prefix = 'VAR - '\n",
    "        elif q in random_queries:\n",
    "            prefix = 'RANDOM - '\n",
    "        else:\n",
    "            prefix = ''\n",
    "        \n",
    "        short_q = q[:48] + '...' if len(q) > 48 else q\n",
    "        query_labels[q] = f\"{prefix}{short_q}\"\n",
    "    \n",
    "    selected_matrix.index = [query_labels[q] for q in selected_matrix.index]\n",
    "    \n",
    "    # Round all numbers to 3 decimal places\n",
    "    selected_matrix = selected_matrix.round(3)\n",
    "    \n",
    "    # Print the table\n",
    "    print(\"\\nQuery Performance Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(selected_matrix.to_string())\n",
    "    print(\"\\nMethod Averages:\")\n",
    "    print(\"-\" * 40)\n",
    "    method_averages = df.groupby('Method')['NDCG'].mean().sort_values(ascending=False)\n",
    "    for method, avg in method_averages.items():\n",
    "        print(f\"{method:15} {avg:.3f}\")\n",
    "\n",
    "# Generate table\n",
    "create_query_table(results, 'benchmark_dim1024_k100_q', 50, 30, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_benchmarks(results_dict, benchmark_keys, names=None, skip_methods=['float'], enable_save_plot=False, name_postfix=\"\", plot_title='NDCG Comparison Across Embedding Models'):\n",
    "    \"\"\"Compare NDCG scores across multiple benchmark results.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary containing multiple benchmark results\n",
    "        benchmark_keys: List of benchmark result keys to compare\n",
    "        names: Optional list of names to use for the benchmarks (e.g. 'BERT-1024', 'BERT-768')\n",
    "        skip_methods: List of methods to exclude from comparison\n",
    "    \"\"\"\n",
    "    if names is None:\n",
    "        names = benchmark_keys\n",
    "        \n",
    "    if len(benchmark_keys) != len(names):\n",
    "        raise ValueError(\"Length of benchmark_keys and names must match\")\n",
    "    \n",
    "    # Collect metadata and basic statistics for each benchmark\n",
    "    benchmark_info = {}\n",
    "    for key, name in zip(benchmark_keys, names):\n",
    "        # Get basic statistics for each method\n",
    "        method_stats = {}\n",
    "        for method, stats in results_dict[key]['summary'].items():\n",
    "            method_stats[method] = {\n",
    "                'mean_time': stats['time_us']['mean'],\n",
    "                'std_time': stats['time_us']['std'],\n",
    "                'ndcg': stats.get('ndcg', {}).get('mean', 1.0),\n",
    "                'ndcg_std': stats.get('ndcg', {}).get('std', 0.0),\n",
    "            }\n",
    "            \n",
    "        benchmark_info[name] = {\n",
    "            'metadata': results_dict[key]['metadata'],\n",
    "            'stats': method_stats\n",
    "        }\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    methods = list(benchmark_info[names[0]]['stats'].keys())\n",
    "    methods_filtered = [m for m in methods if m not in skip_methods]\n",
    "    methods_filtered = [m for m in methods if not any(fnmatch.fnmatch(m, pattern) for pattern in skip_methods)]\n",
    "    x = np.arange(len(methods_filtered))\n",
    "    width = 0.8 / len(names)  # Width of bars\n",
    "    \n",
    "    # Create NDCG comparison plot\n",
    "    plt.figure()\n",
    "    \n",
    "    # Plot bars for each benchmark\n",
    "    for i, name in enumerate(names):\n",
    "        ndcg_scores = [benchmark_info[name]['stats'][method]['ndcg'] \n",
    "                      for method in methods_filtered]\n",
    "        \n",
    "        plt.bar(x + i*width - width*len(names)/2 + width/2,\n",
    "                ndcg_scores,\n",
    "                width,\n",
    "                label=name)\n",
    "    \n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('NDCG Score')\n",
    "    plt.title(plot_title)\n",
    "    plt.xticks(x, methods_filtered, rotation=30, ha='right')\n",
    "    plt.legend(loc=3, bbox_to_anchor=(-0.1,-0.3))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot if enabled\n",
    "    if len(benchmark_keys) > 1:\n",
    "        if enable_save_plot: save_plot(f'benchmark_comparison_{get_title_metadata(result_key=benchmark_keys[0], with_unterscores=True)}_{get_title_metadata(result_key=benchmark_keys[1], with_unterscores=True)}{name_postfix}')\n",
    "    else:\n",
    "        if enable_save_plot: save_plot(f'benchmark_comparison_{get_title_metadata(result_key=benchmark_keys[0], with_unterscores=True)}{name_postfix}')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    print(\"\\nDetailed Comparison:\")\n",
    "    for method in methods:\n",
    "        print(f\"\\nMethod: {method}\")\n",
    "        print(\"-\" * 50)\n",
    "        for name in names:\n",
    "            stats = benchmark_info[name]['stats'][method]\n",
    "            dim = benchmark_info[name]['metadata']['vector_dim']\n",
    "            print(f\"{name} (dim={dim}):\")\n",
    "            print(f\"  Time: {stats['mean_time']:.2f} ± {stats['std_time']:.2f} μs\")\n",
    "            if method != 'float':\n",
    "                print(f\"  NDCG: {stats['ndcg']:.3f} ± {stats['ndcg_std']:.3f}\")\n",
    "\n",
    "# Example usage with multiple benchmark results\n",
    "compare_benchmarks(results, \n",
    "                   benchmark_keys=['benchmark_dim1024_k100_q', 'benchmark_dim768_k100_q'],\n",
    "                   names=['mxbai-embed-large-v1', 'all-mpnet-base-v2'],\n",
    "                   skip_methods=['float', 'twostep_rf5', 'twostep_rf10', 'twostep_rf25', 'ts_mf_rf5', 'ts_mf_rf10', 'ts_mf_rf25'], enable_save_plot=True)\n",
    "compare_benchmarks(results, \n",
    "                   benchmark_keys=['benchmark_dim1024_k100_re', 'benchmark_dim768_k100_re'],\n",
    "                   names=['mxbai-embed-large-v1', 'all-mpnet-base-v2'],\n",
    "                   skip_methods=['float'])\n",
    "compare_benchmarks(results, \n",
    "                   benchmark_keys=['benchmark_dim1024_k100_q', 'benchmark_dim1024_k25_q'],\n",
    "                   names=['k=100', 'k=25'],\n",
    "                   skip_methods=['float', 'twostep_rf5', 'twostep_rf10', 'twostep_rf25', 'ts_mf_rf5', 'ts_mf_rf10', 'ts_mf_rf25'], enable_save_plot=True, plot_title='NDCG Comparison Across k', name_postfix='_cmp_k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_ndcg_scatter_comparison\n",
    "def format_method_name(method):\n",
    "    \"\"\"Format method names for display in plots and tables.\n",
    "    \n",
    "    Args:\n",
    "        method: Original method name\n",
    "    Returns:\n",
    "        Formatted method name\n",
    "    \"\"\"\n",
    "    if method.startswith('twostep_rf'):\n",
    "        return f\"ts_rf{method.split('twostep_rf')[1]}\"\n",
    "    return method\n",
    "\n",
    "def plot_ndcg_scatter_comparison(results_dict, key1, key2, name1=None, name2=None, skip_methods=['float'], filename_suffix:str=None, enable_save_plot=False, name_postfix=\"\"):\n",
    "    \"\"\"Create scatter plot comparing NDCG scores between two benchmarks.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary containing benchmark results\n",
    "        key1: Key of first benchmark result\n",
    "        key2: Key of second benchmark result\n",
    "        name1: Display name for first benchmark (optional)\n",
    "        name2: Display name for second benchmark (optional)\n",
    "        skip_methods: List of methods to exclude from comparison\n",
    "    \"\"\"\n",
    "    if name1 is None:\n",
    "        name1 = key1\n",
    "    if name2 is None:\n",
    "        name2 = key2\n",
    "    \n",
    "    results1 = results_dict[key1]\n",
    "    results2 = results_dict[key2]\n",
    "    \n",
    "    # Collect NDCG scores from both benchmarks\n",
    "    ndcg_data = []\n",
    "    \n",
    "    # Process each run/query\n",
    "    for run1, run2 in zip(results1['runs'], results2['runs']):\n",
    "        query = run1['query_text']\n",
    "        \n",
    "        # Get NDCG scores for each method\n",
    "        for search1, search2 in zip(run1['searches'], run2['searches']):\n",
    "            method = search1['method']\n",
    "            if method not in skip_methods:\n",
    "                ndcg_data.append({\n",
    "                    'Query': query,\n",
    "                    'Method': format_method_name(method),\n",
    "                    'NDCG_1': search1['metrics'].get('ndcg', 1.0),\n",
    "                    'NDCG_2': search2['metrics'].get('ndcg', 1.0)\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(ndcg_data)\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    # Plot diagonal line for reference\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Eq. Prf')\n",
    "    \n",
    "    # Plot points for each method with different colors\n",
    "    for method in df['Method'].unique():\n",
    "        method_data = df[df['Method'] == method]\n",
    "        plt.scatter(method_data['NDCG_1'], \n",
    "                method_data['NDCG_2'], \n",
    "                alpha=0.25, \n",
    "                label=method,\n",
    "                s=10)  # Set point size\n",
    "    \n",
    "    plt.xlabel(f'NDCG Score - {name1}')\n",
    "    plt.ylabel(f'NDCG Score - {name2}')\n",
    "    plt.title('NDCG Score Comparison')\n",
    "    \n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add legend with smaller font\n",
    "    plt.legend(fontsize=8, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if filename_suffix is not None: \n",
    "        filename_suffix = \"_\" + filename_suffix\n",
    "    else:\n",
    "        filename_suffix = \"\"\n",
    "    # Save plot if enabled\n",
    "    if enable_save_plot: save_plot(f'ndcg_scatter_{key1}_vs_{key2}{filename_suffix}{name_postfix}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\nCorrelation between benchmark NDCG scores by method:\")\n",
    "    for method in df['Method'].unique():\n",
    "        method_data = df[df['Method'] == method]\n",
    "        corr = method_data['NDCG_1'].corr(method_data['NDCG_2'])\n",
    "        mean_diff = (method_data['NDCG_2'] - method_data['NDCG_1']).mean()\n",
    "        print(f\"\\n{method}:\")\n",
    "        print(f\"Correlation: {corr:.3f}\")\n",
    "        print(f\"Mean difference ({name2} - {name1}): {mean_diff:.3f}\")\n",
    "\n",
    "# Example usage\n",
    "plot_ndcg_scatter_comparison(results,\n",
    "                             key1='benchmark_dim1024_k100_q', key2='benchmark_dim768_k100_q',\n",
    "                             name1='mxbai-embed-large-v1', name2='all-mpnet-base-v2',\n",
    "                             skip_methods=['float', 'avx2', 'twostep_rf5', 'twostep_rf25', 'twostep_rf50', 'pca2', 'pca4', 'pca8', 'pca16', 'pca32', 'ts_mf_rf2', 'ts_mf_rf5', 'ts_mf_rf10', 'ts_mf_rf25', 'ts_mf_rf50', 'float16'], enable_save_plot=True)\n",
    "plot_ndcg_scatter_comparison(results,\n",
    "                             key1='benchmark_dim1024_k100_q', key2='benchmark_dim768_k100_q',\n",
    "                             name1='mxbai-embed-large-v1', name2='all-mpnet-base-v2',\n",
    "                             skip_methods=['float', 'avx2', 'binary', 'int8', 'float16', 'mf', 'twostep_rf2', 'twostep_rf5', 'twostep_rf10', 'twostep_rf25', 'twostep_rf50'],\n",
    "                             filename_suffix=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_analysis(results_dict, result_key, enable_save_plot=False, name_postfix=\"\"):\n",
    "    \"\"\"Analyze PCA performance across different reduction factors.\n",
    "    \n",
    "    Creates two plots:\n",
    "    1. PCA accuracy vs reduction factor\n",
    "    2. PCA time and speedup vs reduction factor\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary containing benchmark results\n",
    "        result_key: Key of the benchmark results to plot\n",
    "    \"\"\"\n",
    "    results = results_dict[result_key]\n",
    "    \n",
    "    # Collect PCA data\n",
    "    pca_data = []\n",
    "    float_time = results['summary']['float']['time_us']['mean']\n",
    "    \n",
    "    for method, stats in results['summary'].items():\n",
    "        if method.startswith('pca'):\n",
    "            reduction_factor = int(method[3:]) if method[3:].isdigit() else 1\n",
    "            original_dim = results['metadata']['vector_dim']\n",
    "            reduced_dim = original_dim // reduction_factor\n",
    "            \n",
    "            pca_data.append({\n",
    "                'Reduction Factor': reduction_factor,\n",
    "                'Reduced Dimension': reduced_dim,\n",
    "                'NDCG': stats.get('ndcg', {}).get('mean', 1.0),\n",
    "                'NDCG_std': stats.get('ndcg', {}).get('std', 0.0),\n",
    "                'Time (μs)': stats['time_us']['mean'],\n",
    "                'Time_std': stats['time_us']['std'],\n",
    "                'Speedup': float_time / stats['time_us']['mean']\n",
    "            })\n",
    "    \n",
    "    if not pca_data:\n",
    "        print(\"No PCA methods found in the benchmark results\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(pca_data).sort_values('Reduction Factor')\n",
    "    \n",
    "    # Plot 1: NDCG vs Reduction Factor\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.errorbar(df['Reduction Factor'], \n",
    "                df['NDCG'],\n",
    "                yerr=df['NDCG_std'],\n",
    "                marker='o',\n",
    "                capsize=5,\n",
    "                capthick=1,\n",
    "                elinewidth=1,\n",
    "                markersize=8,\n",
    "                color='black')\n",
    "    \n",
    "    plt.xlabel('Reduction Factor')\n",
    "    plt.ylabel('NDCG Score')\n",
    "    plt.title('PCA Accuracy vs Reduction Factor')\n",
    "    \n",
    "    # Add reduced dimension as top axis\n",
    "    ax_top = plt.gca().twiny()\n",
    "    ax_top.set_xlim(plt.gca().get_xlim())\n",
    "    ax_top.set_xticks(df['Reduction Factor'])\n",
    "    ax_top.set_xticklabels([f'{dim}' for dim in df['Reduced Dimension']])\n",
    "    ax_top.set_xlabel('Reduced Dimension')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if enable_save_plot: save_plot(f'pca_accuracy_{result_key}{name_postfix}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 2: Time and Speedup vs Reduction Factor\n",
    "    plt.figure()\n",
    "    \n",
    "    # Plot time\n",
    "    ax1 = plt.gca()\n",
    "    ax1.errorbar(df['Reduction Factor'], \n",
    "                df['Time (μs)'],\n",
    "                yerr=df['Time_std'],\n",
    "                marker='o',\n",
    "                capsize=5,\n",
    "                capthick=1,\n",
    "                elinewidth=1,\n",
    "                markersize=8,\n",
    "                color='black',\n",
    "                label='Time')\n",
    "    \n",
    "    # Add speedup as second y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df['Reduction Factor'], \n",
    "            df['Speedup'],\n",
    "            color='gray',\n",
    "            marker='s',\n",
    "            linestyle='--',\n",
    "            label='Speedup')\n",
    "    \n",
    "    ax1.set_xlabel('Reduction Factor')\n",
    "    ax1.set_ylabel('Search Time (μs)')\n",
    "    ax2.set_ylabel('Speedup vs Float')\n",
    "    plt.title('PCA Performance vs Reduction Factor')\n",
    "    \n",
    "    # Add reduced dimension as top axis\n",
    "    ax_top = ax1.twiny()\n",
    "    ax_top.set_xlim(ax1.get_xlim())\n",
    "    ax_top.set_xticks(df['Reduction Factor'])\n",
    "    ax_top.set_xticklabels([f'{dim}' for dim in df['Reduced Dimension']])\n",
    "    ax_top.set_xlabel('Reduced Dimension')\n",
    "    \n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if enable_save_plot: save_plot(f'pca_performance_{result_key}{name_postfix}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDetailed PCA Statistics:\")\n",
    "    stats_df = df[['Reduction Factor', 'Reduced Dimension', 'NDCG', 'Time (μs)', 'Speedup']]\n",
    "    stats_df = stats_df.set_index('Reduction Factor').round(3)\n",
    "    print(stats_df.to_string())\n",
    "    \n",
    "    # Calculate correlations\n",
    "    print(\"\\nCorrelations with Reduction Factor:\")\n",
    "    correlations = {\n",
    "        'NDCG': df['Reduction Factor'].corr(df['NDCG']),\n",
    "        'Time': df['Reduction Factor'].corr(df['Time (μs)']),\n",
    "        'Speedup': df['Reduction Factor'].corr(df['Speedup'])\n",
    "    }\n",
    "    for metric, corr in correlations.items():\n",
    "        print(f\"{metric}: {corr:.3f}\")\n",
    "\n",
    "\n",
    "plot_pca_analysis(results, 'benchmark_dim1024_k100_re')\n",
    "plot_pca_analysis(results, 'benchmark_dim1024_k100_q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_memory_bandwidth(results_dict, result_key, skip_methods=[], enable_save_plot=False, name_postfix=\"\"):\n",
    "    \"\"\"Calculate and plot approximate memory bandwidth for each method.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary containing benchmark results\n",
    "        result_key: Key of the benchmark results to plot\n",
    "    \"\"\"\n",
    "    results = results_dict[result_key]\n",
    "    num_vectors = results['metadata']['num_vectors']\n",
    "    vector_dim = results['metadata']['vector_dim']\n",
    "    k = results['metadata']['k']\n",
    "    \n",
    "    # Collect bandwidth data\n",
    "    bandwidth_data = []\n",
    "    \n",
    "    for method, stats in results['summary'].items():\n",
    "        if method not in skip_methods:\n",
    "            time_seconds = stats['time_us']['mean'] / 1_000_000  # Convert microseconds to seconds\n",
    "            memory_bytes = calculate_bandwidth_memory(method, num_vectors, vector_dim, k)\n",
    "            \n",
    "            # Calculate bandwidth in GB/s\n",
    "            bandwidth = (memory_bytes / (1024 * 1024 * 1024)) / time_seconds\n",
    "            \n",
    "            bandwidth_data.append({\n",
    "                'Method': format_method_name(method),\n",
    "                'Bandwidth (GB/s)': bandwidth,\n",
    "                'Memory (GB)': memory_bytes / (1024 * 1024 * 1024),\n",
    "                'Time (ms)': stats['time_us']['mean'] / 1000\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(bandwidth_data)\n",
    "\n",
    "    plt.figure()\n",
    "    bars = plt.bar(df['Method'], df['Bandwidth (GB/s)'])\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}',\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=5)\n",
    "    \n",
    "    plt.title('Approximate Memory Bandwidth by Method')\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('Bandwidth (GB/s)')\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if enable_save_plot: save_plot(f'memory_bandwidth_{result_key}{name_postfix}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\nDetailed Bandwidth Analysis:\")\n",
    "    print(\"\\nMethod Statistics:\")\n",
    "    stats_df = df.sort_values('Bandwidth (GB/s)', ascending=False)\n",
    "    print(stats_df.round(2).to_string(index=False))\n",
    "    \n",
    "    # Calculate relative bandwidth compared to float\n",
    "    float_bandwidth = df[df['Method'] == 'float']['Bandwidth (GB/s)'].iloc[0]\n",
    "    print(\"\\nRelative Bandwidth (compared to float):\")\n",
    "    for _, row in df.iterrows():\n",
    "        relative = row['Bandwidth (GB/s)'] / float_bandwidth\n",
    "        print(f\"{row['Method']:15} {relative:.2f}x\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "plot_memory_bandwidth(results, 'benchmark_dim1024_k100_re', ['float16'])\n",
    "plot_memory_bandwidth(results, 'benchmark_dim1024_k100_q', ['float16'])\n",
    "plot_memory_bandwidth(results, 'benchmark_dim1024_k100_q', ['float16', 'twostep_rf5', 'twostep_rf10', 'twostep_rf25', 'ts_mf_rf5', 'ts_mf_rf10', 'ts_mf_rf25'], enable_save_plot=True)\n",
    "plot_memory_bandwidth(results, 'benchmark_dim768_k100_q', ['float16', 'twostep_rf5', 'twostep_rf10', 'twostep_rf25', 'ts_mf_rf5', 'ts_mf_rf10', 'ts_mf_rf25'], enable_save_plot=True)\n",
    "plot_memory_bandwidth(results, 'benchmark_results_1733419058', ['float16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary(results, result_key):\n",
    "    results = results[result_key]\n",
    "    num_vectors = results['metadata']['num_vectors']\n",
    "    vector_dim = results['metadata']['vector_dim']\n",
    "    k = results['metadata']['k']\n",
    "    summary_df = pd.DataFrame([\n",
    "        {\n",
    "            'Method': method,\n",
    "            'Mean Time (ms)': stats['time_us']['mean'] / 1000,\n",
    "            'Std Time': stats['time_us']['std'] / 1000,\n",
    "            'NDCG': stats.get('ndcg', {}).get('mean', 1.0),\n",
    "            'Jaccard': stats.get('jaccard_index', {}).get('mean', 1.0),\n",
    "            'Overlap': stats.get('overlap', {}).get('mean', results['metadata']['k']),\n",
    "            'Memory (GB)' : calculate_method_memory(method, num_vectors, vector_dim) / (1024**3),\n",
    "            'Bandw. (GB/s)': (calculate_bandwidth_memory(method, num_vectors, vector_dim, k) / (stats['time_us']['mean'] * 10**-6)) / (1024**3)\n",
    "        }\n",
    "        for method, stats in results['summary'].items()\n",
    "    ])\n",
    "    \n",
    "    display(Markdown(f\"## Summary for {result_key}\"))\n",
    "    display(Markdown(\"### Metadata\"))\n",
    "    display(Markdown(\"```json\\n\" + json.dumps(results['metadata'], indent=2) + \"\\n```\"))\n",
    "    display(Markdown(\"### Results\"))\n",
    "    display(summary_df.round(3))\n",
    "    \n",
    "show_summary(results, 'benchmark_dim1024_k100_re')\n",
    "show_summary(results, 'benchmark_dim768_k100_re')\n",
    "\n",
    "show_summary(results, 'benchmark_results_1733419058')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def plot_value_distribution(df, enable_save_plot=False, name_postfix=\"\"):\n",
    "    \"\"\"Create visualization of the value distribution with value mapping.\"\"\"\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    # Create plot with two x-axes\n",
    "    ax1 = plt.gca()\n",
    "    ax2 = ax1.twiny()  # Create second x-axis\n",
    "    \n",
    "    # Plot size distribution as a line\n",
    "    ax1.plot(df['Partition'], df['Size'], \n",
    "            color='black', linewidth=1,\n",
    "            alpha=0.8)\n",
    "    \n",
    "    # Add area fill under the line\n",
    "    ax1.fill_between(df['Partition'], df['Size'],\n",
    "                    color='black', alpha=0.1)\n",
    "    \n",
    "    # Set up the second x-axis with values\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    \n",
    "    # Select a subset of partitions for readability\n",
    "    step = 16\n",
    "    selected_partitions = df.iloc[::step]\n",
    "    \n",
    "    ax2.set_xticks(selected_partitions['Partition'])\n",
    "    ax2.set_xticklabels([f'{val:.3f}' for val in selected_partitions['Average']], \n",
    "                        rotation=30, ha='left')\n",
    "    \n",
    "    # Labels and title\n",
    "    ax1.set_title('Value Distribution Across Partitions')\n",
    "    ax1.set_xlabel('Partition Index')\n",
    "    ax2.set_xlabel('Mapped Values')\n",
    "    ax1.set_ylabel('Number of Elements')\n",
    "    ax1.set_ylim(0)\n",
    "    \n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if enable_save_plot: save_plot(f'value_distribution{name_postfix}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDistribution Statistics:\")\n",
    "    print(f\"Total elements: {df['Size'].sum():,}\")\n",
    "    print(f\"Max partition size: {df['Size'].max():,}\")\n",
    "    print(f\"Average partition size: {df['Size'].mean():,.1f}\")\n",
    "    print(f\"Value range: [{df['Range_Start'].min():.6f}, {df['Range_End'].max():.6f}]\")\n",
    "\n",
    "# Load and parse data\n",
    "def parse_partition_data(file_path):\n",
    "    \"\"\"Parse the partition data from a file into a DataFrame.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Extract partition lines\n",
    "    lines = text.split('\\n')\n",
    "    partition_data = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip non-partition lines\n",
    "        if not line.strip().startswith('Partition'):\n",
    "            continue\n",
    "            \n",
    "        # Extract values using regex\n",
    "        match = re.search(r'Partition\\s+(\\d+):\\s+size\\s+=\\s+(\\d+)\\s+elements,\\s+range\\s+\\[(.*?),\\s+(.*?)\\],\\s+avg\\s+=\\s+(.*?)$', line)\n",
    "        if match:\n",
    "            partition_data.append({\n",
    "                'Partition': int(match.group(1)),\n",
    "                'Size': int(match.group(2)),\n",
    "                'Range_Start': float(match.group(3)),\n",
    "                'Range_End': float(match.group(4)),\n",
    "                'Average': float(match.group(5))\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(partition_data)\n",
    "\n",
    "# Generate plot\n",
    "df_partitions = parse_partition_data('results/mapped_float_partitions.txt')\n",
    "df_partitions768 = parse_partition_data('results/mapped_float_partitions_768.txt')\n",
    "plot_value_distribution(df_partitions, enable_save_plot=True)\n",
    "plot_value_distribution(df_partitions768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_distribution(df):\n",
    "    \"\"\"Plot cumulative distribution of values.\"\"\"\n",
    "    plt.figure()\n",
    "    \n",
    "    cumsum = df['Size'].cumsum()\n",
    "    cumsum_normalized = cumsum / cumsum.max()\n",
    "    \n",
    "    plt.plot(df['Average'], cumsum_normalized, \n",
    "             color='black', linewidth=1)\n",
    "    \n",
    "    plt.title('Cumulative Distribution')\n",
    "    plt.xlabel('Mapped Float Value')\n",
    "    plt.ylabel('Cumulative Proportion')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_cumulative_distribution(df_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quantization_analysis(df, enable_save_plot=False, name_postfix=\"\"):\n",
    "    \"\"\"Plot the value ranges and averages to show quantization.\"\"\"\n",
    "    plt.figure()\n",
    "    \n",
    "    # Plot ranges as vertical lines\n",
    "    for _, row in df.iterrows():\n",
    "        plt.vlines(x=row['Partition'], \n",
    "                  ymin=row['Range_Start'], \n",
    "                  ymax=row['Range_End'],\n",
    "                  color='blue', alpha=0.6)\n",
    "    \n",
    "    # Plot averages as points\n",
    "    plt.scatter(df['Partition'], df['Average'], \n",
    "               color='black', s=10, alpha=0.3)\n",
    "    \n",
    "    plt.title('Quantization Mapping')\n",
    "    plt.xlabel('Int8 Value (Partition)')\n",
    "    plt.ylabel('Float32 Value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    if enable_save_plot: save_plot(f'quantization_mapping{name_postfix}')\n",
    "    plt.show()\n",
    "    \n",
    "plot_quantization_analysis(df_partitions ,enable_save_plot=True)\n",
    "plot_quantization_analysis(df_partitions768, enable_save_plot=True, name_postfix=\"_768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_density(df):\n",
    "    \"\"\"Create a density plot showing concentration of values.\"\"\"\n",
    "    plt.figure()\n",
    "    \n",
    "    # Calculate density (elements per value range)\n",
    "    df['Range_Size'] = df['Range_End'] - df['Range_Start']\n",
    "    df['Density'] = df['Size'] / df['Range_Size']\n",
    "    \n",
    "    plt.plot(df['Average'], df['Density'], \n",
    "             color='black', linewidth=1)\n",
    "    \n",
    "    plt.title('Value Density Distribution')\n",
    "    plt.xlabel('Float32 Value')\n",
    "    plt.ylabel('Elements per Value Unit')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_value_density(df_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_analysis(df):\n",
    "    \"\"\"Create a combined plot showing multiple aspects of the quantization.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "    \n",
    "    # Top plot: Distribution\n",
    "    ax1.bar(df['Partition'], df['Size'], \n",
    "            alpha=0.6, color='black', width=1.0)\n",
    "    ax1.set_title('Element Distribution')\n",
    "    ax1.set_ylabel('Number of Elements')\n",
    "    \n",
    "    # Bottom plot: Quantization mapping\n",
    "    ax2.scatter(df['Partition'], df['Average'], \n",
    "                color='black', s=20, alpha=0.6)\n",
    "    ax2.vlines(df['Partition'], \n",
    "               df['Range_Start'], df['Range_End'],\n",
    "               color='gray', alpha=0.3)\n",
    "    ax2.set_xlabel('Int8 Value (Partition)')\n",
    "    ax2.set_ylabel('Float32 Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_combined_analysis(df_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_partition_efficiency(df):\n",
    "    \"\"\"Analyze how effectively each partition captures similar values.\"\"\"\n",
    "    plt.figure()\n",
    "    \n",
    "    # Calculate variance in each partition\n",
    "    df['Value_Range'] = df['Range_End'] - df['Range_Start']\n",
    "    df['Elements_per_Range'] = df['Size'] / df['Value_Range']\n",
    "    \n",
    "    plt.scatter(df['Average'], df['Elements_per_Range'],\n",
    "                color='black', alpha=0.6, s=20)\n",
    "    \n",
    "    plt.title('Partition Efficiency')\n",
    "    plt.xlabel('Float32 Value')\n",
    "    plt.ylabel('Elements per Value Range')\n",
    "    plt.yscale('log')  # Log scale might be better for this\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_partition_efficiency(df_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distribution_stats(df):\n",
    "    \"\"\"Calculate and print basic distribution statistics.\"\"\"\n",
    "    stats = {\n",
    "        'Total Elements': df['Size'].sum(),\n",
    "        'Mean Elements per Partition': df['Size'].mean(),\n",
    "        'Median Elements per Partition': df['Size'].median(),\n",
    "        'Std Dev Elements': df['Size'].std(),\n",
    "        'Skewness': df['Size'].skew(),\n",
    "        'Kurtosis': df['Size'].kurtosis(),\n",
    "        \n",
    "        # Value range statistics\n",
    "        'Total Value Range': df['Range_End'].max() - df['Range_Start'].min(),\n",
    "        'Mean Value Range per Partition': (df['Range_End'] - df['Range_Start']).mean(),\n",
    "        'Mean Absolute Value': abs(df['Average']).mean(),\n",
    "        \n",
    "        # Quantization statistics\n",
    "        'Average Quantization Step': (df['Range_End'] - df['Range_Start']).mean(),\n",
    "        'Max Quantization Error': (df['Range_End'] - df['Range_Start']).max() / 2,\n",
    "        'Min Quantization Error': (df['Range_End'] - df['Range_Start']).min() / 2\n",
    "    }\n",
    "    \n",
    "    # Print formatted statistics\n",
    "    print(\"\\nDistribution Statistics:\")\n",
    "    for name, value in stats.items():\n",
    "        print(f\"{name:30s}: {value:,.6f}\")\n",
    "        \n",
    "calculate_distribution_stats(df_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_partition_balance(df):\n",
    "    \"\"\"Analyze how well balanced the partitions are.\"\"\"\n",
    "    # Calculate partition utilization\n",
    "    total_elements = df['Size'].sum()\n",
    "    df['Utilization'] = df['Size'] / total_elements * 100\n",
    "    \n",
    "    stats = {\n",
    "        'Most Populated Partition': df.loc[df['Size'].idxmax(), 'Partition'],\n",
    "        'Max Partition Utilization %': df['Utilization'].max(),\n",
    "        'Min Partition Utilization %': df['Utilization'].min(),\n",
    "        'Utilization Std Dev %': df['Utilization'].std(),\n",
    "        'Empty Partitions': (df['Size'] == 0).sum(),\n",
    "        'Effective Partitions': (df['Size'] > 0).sum()\n",
    "        #'Gini Coefficient': gini_coefficient(df['Size'])  # Need to implement this\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPartition Balance Analysis:\")\n",
    "    for name, value in stats.items():\n",
    "        print(f\"{name:30s}: {value:,.6f}\")\n",
    "        \n",
    "analyze_partition_balance(df_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_value_ranges(df):\n",
    "    \"\"\"Analyze the distribution of values and ranges.\"\"\"\n",
    "    df['Range_Size'] = df['Range_End'] - df['Range_Start']\n",
    "    df['Density'] = df['Size'] / df['Range_Size']\n",
    "    \n",
    "    stats = {\n",
    "        'Positive Values %': (df[df['Average'] > 0]['Size'].sum() / df['Size'].sum()) * 100,\n",
    "        'Negative Values %': (df[df['Average'] < 0]['Size'].sum() / df['Size'].sum()) * 100,\n",
    "        'Mean Value Density': df['Density'].mean(),\n",
    "        'Max Value Density': df['Density'].max(),\n",
    "        'Value Range Coverage %': (df[df['Size'] > 0]['Range_Size'].sum() / \n",
    "                                 (df['Range_End'].max() - df['Range_Start'].min())) * 100\n",
    "    }\n",
    "    \n",
    "    print(\"\\nValue Range Analysis:\")\n",
    "    for name, value in stats.items():\n",
    "        print(f\"{name:30s}: {value:,.6f}\")\n",
    "        \n",
    "analyze_value_ranges(df_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_quantization_error(df):\n",
    "    \"\"\"Analyze the potential quantization errors.\"\"\"\n",
    "    df['Max_Error'] = (df['Range_End'] - df['Range_Start']) / 2\n",
    "    df['Weighted_Error'] = df['Max_Error'] * df['Size']\n",
    "    \n",
    "    stats = {\n",
    "        'Average Max Error': df['Weighted_Error'].sum() / df['Size'].sum(),\n",
    "        'Worst Case Error': df['Max_Error'].max(),\n",
    "        'Best Case Error': df['Max_Error'].min(),\n",
    "        'Error Std Dev': df['Max_Error'].std(),\n",
    "        'Mean Relative Error %': (df['Max_Error'] / df['Average'].abs()).mean() * 100\n",
    "    }\n",
    "    \n",
    "    print(\"\\nQuantization Error Analysis:\")\n",
    "    for name, value in stats.items():\n",
    "        print(f\"{name:30s}: {value:,.6f}\")\n",
    "        \n",
    "analyze_quantization_error(df_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_distribution_shape(df):\n",
    "    \"\"\"Analyze the shape and characteristics of the distribution.\"\"\"\n",
    "    # Calculate percentiles\n",
    "    percentiles = [10, 25, 50, 75, 90]\n",
    "    value_percentiles = np.percentile(df['Average'], percentiles)\n",
    "    size_percentiles = np.percentile(df['Size'], percentiles)\n",
    "    \n",
    "    stats = {\n",
    "        'Distribution Mode': df.loc[df['Size'].idxmax(), 'Average'],\n",
    "        'Value Range 90%': value_percentiles[-1] - value_percentiles[0],\n",
    "        'Size Range 90%': size_percentiles[-1] - size_percentiles[0],\n",
    "        'Interquartile Range': value_percentiles[3] - value_percentiles[1],\n",
    "        'Value Concentration': (df[df['Size'] > df['Size'].mean()]['Size'].sum() / \n",
    "                              df['Size'].sum() * 100)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nDistribution Shape Analysis:\")\n",
    "    for name, value in stats.items():\n",
    "        print(f\"{name:30s}: {value:,.6f}\")\n",
    "        \n",
    "analyze_distribution_shape(df_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_statistical_summary(df, enable_save_plot=False, name_postfix=\"\"):\n",
    "    \"\"\"Create a comprehensive statistical summary visualization.\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    \n",
    "    # 1. Distribution with percentiles - use weights directly\n",
    "    sns.histplot(data=df, x='Average', weights='Size', \n",
    "                ax=ax1, color='black', alpha=0.6)\n",
    "    \n",
    "    # Calculate weighted percentiles without creating huge arrays\n",
    "    percentiles = [25, 50, 75]\n",
    "    total = df['Size'].sum()\n",
    "    cumsum = 0\n",
    "    percentile_values = []\n",
    "    \n",
    "    # Sort by Average and calculate weighted percentiles\n",
    "    df_sorted = df.sort_values('Average')\n",
    "    for idx, row in df_sorted.iterrows():\n",
    "        cumsum += row['Size']\n",
    "        for p in percentiles:\n",
    "            if len(percentile_values) < len(percentiles) and cumsum/total > p/100:\n",
    "                percentile_values.append(row['Average'])\n",
    "    \n",
    "    # Add percentile lines\n",
    "    for p, val in zip(percentiles, percentile_values):\n",
    "        ax1.axvline(val, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax1.text(val, ax1.get_ylim()[1], f'P{p}', rotation=90, va='top')\n",
    "    \n",
    "    ax1.set_title('Value Distribution with Percentiles')\n",
    "    \n",
    "    # 2. QQ Plot - use sampling instead of full data\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Sample weighted values (much smaller sample size)\n",
    "    sample_size = 10000\n",
    "    probs = df['Size'] / df['Size'].sum()\n",
    "    sampled_indices = np.random.choice(len(df), size=sample_size, p=probs)\n",
    "    sampled_values = df['Average'].iloc[sampled_indices]\n",
    "    \n",
    "    qq = stats.probplot(sampled_values, dist=\"norm\", plot=ax2)\n",
    "    ax2.set_title('Quantile-Quantile Plot (Sampled)')\n",
    "    \n",
    "    # 3. Box plot of partition sizes\n",
    "    sns.boxplot(y=df['Size'], ax=ax3, color='lightgray')\n",
    "    ax3.set_title('Partition Size Distribution')\n",
    "    \n",
    "    # 4. Cumulative density\n",
    "    cumsum = (df['Size'].cumsum() / df['Size'].sum())\n",
    "    ax4.plot([0, 1], [0, 1], 'r--', alpha=0.5, label='Perfect Equality')\n",
    "    ax4.plot(np.linspace(0, 1, len(cumsum)), cumsum, 'black', label='Actual Distribution')\n",
    "    ax4.set_title('Lorenz Curve (Distribution Equality)')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if enable_save_plot: save_plot(f'statistical_summary{name_postfix}')\n",
    "    plt.show()\n",
    "    \n",
    "plot_statistical_summary(df_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_analysis(df):\n",
    "    \"\"\"Perform statistical tests using sampling to reduce memory usage.\"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Sample size for statistical tests\n",
    "    sample_size = 10000\n",
    "    \n",
    "    # Normalize probabilities to ensure they sum to 1\n",
    "    probs = df['Size'] / df['Size'].sum()\n",
    "    probs = probs.values  # Convert to numpy array\n",
    "    probs = probs / probs.sum()  # Renormalize to ensure sum is exactly 1\n",
    "    \n",
    "    # Create weighted sample\n",
    "    samples = np.random.choice(df['Average'].values, size=sample_size, p=probs)\n",
    "    \n",
    "    # 1. Normality Tests\n",
    "    normality_tests = {\n",
    "        'Shapiro-Wilk Test': stats.shapiro(samples),\n",
    "        'D\\'Agostino K^2 Test': stats.normaltest(samples)\n",
    "        # Anderson-Darling handled separately\n",
    "    }\n",
    "    \n",
    "    # 2. Distribution Fitting\n",
    "    distributions = ['norm', 'laplace', 'logistic']\n",
    "    dist_fits = {}\n",
    "    \n",
    "    for dist_name in distributions:\n",
    "        dist = getattr(stats, dist_name)\n",
    "        params = dist.fit(samples)\n",
    "        d_stat, p_val = stats.kstest(samples, dist_name, params)\n",
    "        dist_fits[dist_name] = {\n",
    "            'parameters': params,\n",
    "            'ks_statistic': d_stat,\n",
    "            'p_value': p_val\n",
    "        }\n",
    "    \n",
    "    # 3. Shape Analysis\n",
    "    shape_stats = {\n",
    "        'Skewness': stats.skew(samples),\n",
    "        'Kurtosis': stats.kurtosis(samples),\n",
    "        'Variance Ratio': df['Size'].var() / df['Size'].mean(),\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nNormality Tests (based on sampling):\")\n",
    "    for test_name, result in normality_tests.items():\n",
    "        print(f\"{test_name}:\")\n",
    "        print(f\"  Statistic: {float(result[0]):.6f}\")\n",
    "        print(f\"  p-value: {float(result[1]):.6f}\")\n",
    "    \n",
    "    # Handle Anderson-Darling test separately\n",
    "    ad_result = stats.anderson(samples)\n",
    "    print(\"\\nAnderson-Darling Test:\")\n",
    "    print(f\"  Statistic: {float(ad_result[0]):.6f}\")\n",
    "    print(\"  Critical Values:\")\n",
    "    for sig_level, crit_val in zip(ad_result[1], ad_result[2]):\n",
    "        print(f\"    {sig_level}%: {crit_val:.6f}\")\n",
    "    \n",
    "    print(\"\\nDistribution Fitting:\")\n",
    "    for dist_name, result in dist_fits.items():\n",
    "        print(f\"\\n{dist_name} distribution:\")\n",
    "        print(f\"  KS statistic: {result['ks_statistic']:.6f}\")\n",
    "        print(f\"  p-value: {result['p_value']:.6f}\")\n",
    "        print(f\"  Parameters: {', '.join(f'{float(p):.6f}' for p in result['parameters'])}\")\n",
    "    \n",
    "    print(\"\\nShape Analysis:\")\n",
    "    for stat_name, value in shape_stats.items():\n",
    "        print(f\"{stat_name}: {float(value):.6f}\")\n",
    "\n",
    "\n",
    "#plot_statistical_summary(df_partitions)\n",
    "perform_statistical_analysis(df_partitions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
