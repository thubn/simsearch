% introduction.tex
\chapter{Introduction}
\label{introduction}
This chapter creates and overview of Vector Similarity Search, its importance in the data driven world,
and outlines the motivation for this research.

\section{Motivation}
\subsection{The Power of Vector Similarity Search}
Everyday massive amounts of digital data like texts, documents, images, videos, etc. are created. Tomorrow even more.
A large part of that data is unstructured, which presents a challenge: How can we search efficiently through this growing amount of information?
The traditional way would be using keyword-based search, where the data is searched for matching words or strings. Like searching for a text passage in a document or searching for file by its name.
However, this method only works on text data and not on a semantic level.

Therefore, vector similarity search can play an important role.
It is able to compare the semantic meaning of a text based search queries to a database of images, texts, etc.
This enables you, for example, to search a local photo gallery for pictures containing cats with the simple query 'cat'. Vector Similarity Search works by converting the input data (texts, images, ...) into embeddings,
which are just high dimensional - numerical vectors.
Encoded in these vectors are for example the meaning of its encoded text
or the visual details of a picture.
Recommendation systems of streaming platforms~\cite{6213086}, social networks~\cite{ozsoy2016wordembeddingsitemrecommendation} and ecommerce~\cite{8821893} platforms also often use vector similarity search. It can even be used for machine language translation.~\cite{mikolov2013exploitingsimilaritieslanguagesmachine}

To experience the results of this thesis first hand, the reader can download the project from git which includes a small python web server to search for Wikipedia articles using the implementations presented in this thesis. More info in \autoref{sec:tryingout}.

\subsection{The need for optimization}
Consider a mobile photo gallery with 10,000 images. Each embedding requires 4 KB of memory, when using the float32 format. This would total 40 MB of memory use. An offline machine translator with 250,000 translations would require 1 GB of memory. With a typical device having only 4 GB of RAM, searching these embeddings could become slow and memory intensive.
Now that our world is becoming increasingly mobile first a new problem arises.
Because Vector Similarity Search is currently mostly run in datacenters, where the amount of RAM starts to get into the Terabyte territory, it is very expensive on resources like processing power and memory.
But there are valid reasons for running vector similarity search on resource constrained mobile devices.
Like offline gallery search without the need to share all data with a cloud provider to offload computation onto the datacenters. Another use can be smart home devices, where the device matches the semantic meaning of voice commands to actions. Here latency would be very important for natural interaction.
Embedded systems in vehicles can use vector similarity search for object detection and classification. Which also requires low latency and high reliability, while being resource constrained.

This creates the fundamental question: How can we perform vector similarity search on devices with limited resources while maintaining as much speed and accuracy as possible?

\section{Research Objectives}
The objective of this thesis is to addresses the previously mentioned optimization challenges for resource constrained devices by investigating and developing optimization techniques for Vector Similarity Search.
The following approaches will be implemented, fine-tuned and compared to each other by evaluating their performance (speed, accuracy and memory usage):

\begin{itemize}
    \item Quantization methods which can decrease memory usage and increase speed at the cost of accuracy
    \item Vector dimension reduction also can decrease memory usage and increase speed at the cost of accuracy
    \item Two-Step approach to balance speed and accuracy
    \item SIMD optimization to accelerate calculations by using modern hardware features
\end{itemize}
After this it should be possible to evaluate the effectiveness of these approaches and decide what is actually useful for a specific use case.

%\section{Background on Vector Similarity Search}
%\subsection{Introduction to Embeddings}
%Vector Similarity Search is done on Embeddings.
%Embeddings are numerical representations of data like text, images, audio, etc.
%They represent the semantic meaning of the embedded data in an n-dimensional vectorspace.
%These vectors can have from 128 up to 8192 dimensions for recent embedding models.
%\subsection{Concept of Vector Similarity Search}

\section{Structure}
In \autoref{introduction} an introduction to vector similarity search by describing its use cases and the importance of optimization is given. Additionally, the goals of this thesis are presented.

Chapter \Ref{chapter:kap2} gives a more in depth explanation on what exactly vector embeddings are, their capabilities and how the input data is encoded. Then the mathematical foundations used in the similarity computation are explained too. Next, hardware characteristics and capabilities that are important for similarity search are introduced. Finally, the metrics evaluating the approaches performance will be explained.

Chapter \Ref{chapter:implementation} gives a general overview of the software architecture that the different implementations will share.

In \autoref{optapproaches} the implementation of each approach is presented. Additionally, it shows the progress for some methods from no optimizations to full optimizations. This way the decision on why something is done in a certain way becomes more clear.

Chapter \ref{chapter:exp_eval} gives the parameters of the test environment first and then presents and analyzes the results with the help of plots generated from the recorded metrics.

In \autoref{chapter:future_work} potential new research that could be done with the insights gained in this thesis are outlined.

Lastly, \autoref{chapter:conclusion} wraps this thesis up by summarizing the results and potential new questions that have surfaced from this research.