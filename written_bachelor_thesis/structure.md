# Vector Similarity Search Optimization for Resource-Constrained Devices
## 1. Introduction
- Background on vector sim search and its applications
    - Introduction
        - Embeddings are numerical representations of data (Text, Images, ...)
        - Represented by high-dimensional Vectors (64 up 8192 dimensions and beyond)
        - Semantic meaning is represented by Vectors
        - Vectors/Embeddings are generated by neural Networks
    - Concept of Vector Similarity Search
        - How Vectors closest to a query Vector are found
        - Search is just approximate
        - very different to string matching search
        - accuracy vs speed
        - scaling with very large datasets
            - methods for measuring similarity
                - Dot Product
                - Cosine Similarity
                    - good for text
                - Eucliadian distance
                - Manhatten distance
- Challenges in resource-constrained environments
    - memory might not be enought to have all the embeddings loaded into
    - processing power is not enought to get results in acceptable time
    - finite amount of energy on battery powered devices
    - the algorithms should be implemented in low level languages to make room for optimazations and make good use of the given hardware
- Motivation for optimization
    - optimazation enables lower powered devices to do vector similarity search
    - using less resources like power/memory/smaller processors is alsways better for the envoirement
- Research objectives and contributions
- Thesis structure overview

## 2. Theoretical Background
### 2.1 Vector Similarity Search
- Vector embeddings and their properties
- Cosine similarity and other similarity metrics
- Applications in real-world scenarios (e.g., recommendation systems, semantic search)

### 2.2 Hardware Considerations
- Resource constraints on mobile devices
- SIMD instructions and AVX2
    - SIMD (Single In Multiple Data) enables parallel vector processing
    - With AVX2 you can for example multiply two eight dimensional float vectors with each other in one instruction
- Memory hierarchy and access patterns
    - Flash Memory -> System Memory -> L3 Cache -> L2 Cache -> L1 Cache -> CPU Registers
    - Query gets compared to every Embedding
        - Query most likely stays in cpu cache at all times
        - Embeddings are loaded sequentially from memory
            - Predictable memory acces enables the possibility to prefetch the next vectors while computing similarity of current vectors
- CPU cache considerations
    - CPU Cache is limited
    - Cache pollution by prefetching to much data has to be avoided

### 2.3 Evaluation Metrics
- Jaccard Index
    - Mathematical definition
    - Properties and interpretation
    - Why its suitable to benchmark results
    - Limitation and considerations
- NDCG (Normaized Discounted Cumulative Gain)
    - Mathematical definition
    - Importance of result ranking
    - Why its relevant for similarity search
    - Relation between position and relevance
    - Advantages over simple metrics (ie Jaccard Index)

## 3. Optimization Approaches
### 3.1 Baseline Implementation
- Naive float32 implementation
    - show algorithm
    - consists of 2 nested loops
        - outer iterates over the embeddings
        - inner iterates over the elements of the vectors
            - multiply vectors elements
- Performance characteristics and limitations
    - is the slowest method tested
    - no memory savings
    - a lot of potential of the hardware is unused

### 3.2 Quantization Methods
- Int8 quantization
  - Theory and implementation
    - uses only 8 bit instead of 32 bit for each vector element -> 1/4th of memory is used
    - float embeddings should be normailized first, then the float values (which are between -1 an 1) are multiplied by 127
    - multiplication product of the dot product has to be stored in int16
  - Trade-offs between accuracy and performance
    - uses 1/4th of the memory for the embeddings
    - but still only 2* performance gain compared to float32, because the results have to be store in 16bit
    - decent accuracy
- Binary quantization
  - Sign-based binary conversion
    - just needs to store the sign bit of the original vector elemtents
    - results in fast conversion
  - XNOR and popcount operations
    - very fast binary vector multiplication by using xnor, dot product is built by using popcount on the result
  - Efficiency considerations

### 3.3 Dimension Reduction
- PCA theory and implementation
- Impact of different reduction factors (2x, 4x, 8x, 16x, 32x)
- Trade-offs between dimensionality and accuracy

### 3.4 SIMD Optimization
- AVX2 implementation details
- Memory alignment considerations
    - uses aligned memory for storing Embedding vectors, allows faster loading of the vectors into avx registers
- Loop unrolling and prefetching strategies
    - unrolling loops to allow the cpu make better use of pipelining
- Optimized popcount implementations
    - use avx2 optimized popcount with lookup table
        - is way faster than the builtin cpu popcount, because it only allows to calculate the popcount of 64bit values

### 3.5 Two-Step Search with Rescoring
- Binary search for initial filtering
    - use binary sim search on all embeddings to filter out embeddings with bad scores
    - is ~30 times faster than full precision scoring
- Full-precision rescoring
    - only do full accuracy search on the top results from binary sim search
        - doing full accuracy search on thousands of embeddings is very fast
- Parameter tuning (rescoring factor)
    - binary search gives better results for longer queries
        - longer queries need lower rescoring factors than short queries

## 4. Implementation
### 4.1 Software Architecture
- Overall software design
- Class hierarchy and components
- Memory management strategies

### 4.2 Optimizations
- Detailed discussion of implementation choices
- Code examples for critical sections
    - show cosine/dot product calculation for float, int8 and binary implementations
- Optimization techniques employed
    - loop unrolling
    - prefetching

## 5. Experimental Evaluation
### 5.1 Methodology
- Dataset description
    - currently using 1.2M wikipedia articles
        - embeddings created with `mixedbread-ai/mxbai-embed-large-v1` model
            - results in vectors with 1024 dimensions for each embedding
            - embeddings use 4,9152 GB of memory for full accuracy
            - 1,2288 GB for int8
            - 153,6 MB for binary
    - may upgrade it to 6M wikipedia articles
- Evaluation metrics (time, accuracy, memory usage, benchmarking quantization techniques)
    - Implementation of metrics calculation
    - Reason for using JI and NDCG
    - measure time taken by the `similarity_search()` function and ints underlying `cosine_similarity()` function
    - theoretical memory usage
    - memory usage measure with valgring/massif tool
- Testing environment
    - specify details about used hardware and software

### 5.2 Results and Analysis
- Performance comparisons
- Accuracy analysis (Jaccard Index, NDCG)
- Memory usage analysis
- Discussion of trade-offs

### 5.3 Real-world Applications
- Practical implications
- Use case scenarios
    - semantic search in the photo gallery on smartphone
    - search engines
    - searching for documents
    - ...
- Deployment considerations

## 6. Future Work
- Proposed disk-based binary search extension
    - only load binary embeddings into memory
    - only load top embeddings found by binary sim search into memory for full accuracy search
- Additional optimization possibilities
    - future hardware will have support for float16 which should give a decent performance boost and most likely have 95%+ accuracy
        - can be implemented in software to evaluate the accuracy, but will be slow
- Research directions

## 7. Conclusion
- Summary of contributions
- Key findings and insights
- Recommendations for practical applications

## Appendices
- Detailed benchmark results
- Implementation details
- Additional technical documentation

## Bibliography

# Notes

Looking at your main.pdf file, I can see you've actually covered the key theoretical concepts quite well. Let me analyze what you have and what might still be valuable to add:

Currently covered in main.pdf:

1. Vector Similarity Search (2.1)
- ✓ Vector embeddings and properties 
- ✓ Dot product explanation
- ✓ Cosine similarity with mathematical foundation
- ✓ Real world scenarios (though brief)

2. Hardware Considerations (2.2)
- ✓ Resource constraints mentioned
- ✓ SIMD/AVX2 explained well
- ✓ Memory hierarchy covered
- ✓ Cache considerations discussed

3. Evaluation Metrics (2.3)
- ✓ Jaccard Index fully explained
- ✓ NDCG thoroughly covered with mathematical foundation
- ✓ Good explanation of why these metrics are relevant

Potential additions that could strengthen these sections:

1. For Vector Similarity Search (2.1):
```
2.1.1 Vector embeddings and their properties
[Add a concrete example]
Text: "Hello world" → [0.1, -0.3, 0.8, ..., 0.4]
Show how two semantically similar phrases map to similar vectors
This makes the concept more tangible for readers
```

2. For Hardware Considerations (2.2):
```
2.2.1 Resource constraints on mobile devices
[Add specific numbers]
- Typical server: 256GB+ RAM, 64+ cores
- High-end phone: 8GB RAM, 8 cores
- Mid-range phone: 4GB RAM, 6 cores
This concrete comparison helps justify the need for optimization
```

3. For Memory Hierarchy (2.2.3):
```
[Add timing information]
L1 Cache: ~4 cycles (~1ns)
L2 Cache: ~12 cycles (~3ns)
L3 Cache: ~40 cycles (~10ns)
RAM: ~200-300 cycles (~50-100ns)
This helps readers understand why memory access patterns matter
```

However, these are enhancements rather than essential missing pieces. Your current theoretical foundation is solid and provides sufficient background for understanding your optimization work.

The chapter effectively:
1. Explains the fundamental concepts
2. Provides mathematical foundations where needed
3. Explains the hardware context
4. Justifies your evaluation methods

I would suggest moving forward to documenting your system design and optimizations rather than spending more time on the theoretical background. The additions suggested above could be made if you have time later, but they're not critical to the thesis's completeness.

Would you like to move forward with documenting your system architecture or would you still like to enhance any of the theoretical sections?